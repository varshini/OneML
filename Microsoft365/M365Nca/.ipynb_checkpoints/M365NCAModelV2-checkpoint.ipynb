{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "Index(['O365MAU', 'O365MauTrend', 'O365MauMean', 'EXOMAU', 'EXOMAUTrend',\n",
      "       'EXOMAUMean', 'ODSPMAU', 'ODSPMAUTrend', 'ODSPMAUMean', 'SfBMAU',\n",
      "       ...\n",
      "       'WindowsDeviceLicensingYr1', 'WindowsDeviceLicensingYr2',\n",
      "       'AzureYoYChange', 'AzureMean', 'AzureYr1', 'AzureYr2',\n",
      "       'dynamicsYoYChange', 'dynamicsMean', 'dynamicsYr1', 'dynamicsYr2'],\n",
      "      dtype='object', length=336)\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "#Reading the data\n",
    "training_examples = pandas.read_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Data\\\\Features\\\\Training\\\\AllFeaturesv4.csv\")\n",
    "training_examples = training_examples.drop('CustomerAdd.y', axis=1)\n",
    "training_examples = training_examples.drop('ym', axis=1)\n",
    "training_examples_tpid = training_examples[[\"FinalTPID\"]]\n",
    "training_examples = training_examples.drop('FinalTPID', axis=1)\n",
    "\n",
    "training_labels = training_examples[[\"CustomerAdd.x\"]]\n",
    "training_examples = training_examples.drop('CustomerAdd.x', axis=1)\n",
    "\n",
    "#training_examples = training_examples.iloc[:,0:200]\n",
    "print(len(training_examples.columns))\n",
    "print (training_examples.columns)\n",
    "\n",
    "# Adding some new % features \n",
    "training_examples['AnnuityPercentage'] = training_examples['Annuity']/(training_examples['Annuity'] + training_examples['NonAnnuity']\n",
    "                                                                      + training_examples['DarkAnnuity'])\n",
    "training_examples['NonAnnuityPercentage'] = training_examples['NonAnnuity']/(training_examples['Annuity'] + training_examples['NonAnnuity']\n",
    "                                                                      + training_examples['DarkAnnuity'])\n",
    "training_examples['DarkAnnuityPercentage'] = training_examples['DarkAnnuity']/(training_examples['Annuity'] + training_examples['NonAnnuity']\n",
    "                                                                      + training_examples['DarkAnnuity'])\n",
    "\n",
    "training_examples = training_examples.replace([np.inf, -np.inf], 0)\n",
    "training_examples = training_examples.replace(np.nan, 0)\n",
    "print(len(training_examples.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "322\n"
     ]
    }
   ],
   "source": [
    "# Choose features based on intuition # \n",
    "\n",
    "include_features_flag = pandas.read_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\Tries\\\\FeatureChoice.csv\")\n",
    "included_flag = include_features_flag['Include'] == 1\n",
    "chosen_features = list((include_features_flag[included_flag])[\"Variable\"])\n",
    "print (len(chosen_features))\n",
    "training_examples = training_examples[chosen_features]\n",
    "print (len(training_examples.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['O365MAU', 'O365MauTrend', 'EXOMAUTrend', 'ODSPMAUTrend', 'SfBMAU',\n",
      "       'SfBMAUTrend', 'TeamsMAU', 'TeamsMAUTrend', 'TeamsMAUMean',\n",
      "       'DesktopPerpetual',\n",
      "       ...\n",
      "       'dynamicsYoYChange', 'dynamicsMean', 'dynamicsYr1', 'dynamicsYr2',\n",
      "       'AnnuityPercentage', 'NonAnnuityPercentage', 'DarkAnnuityPercentage',\n",
      "       'EMSE3CaoTotalEMS', 'EMSE3FuslTotalEMS', 'EMSE5CaoTotalEMS'],\n",
      "      dtype='object', length=259)\n"
     ]
    }
   ],
   "source": [
    "# Check variable correlations to each other and removing variables having correlations > 0.9 #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "corr = training_examples.corr() \n",
    "# sns.heatmap(corr)\n",
    "\n",
    "# Remove one of the two variables with correlations > 0.9\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "\n",
    "selected_columns = training_examples.columns[columns]\n",
    "print (selected_columns)\n",
    "training_examples = training_examples[selected_columns]\n",
    "training_examples.to_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\Tries\\\\SelectedCols.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8733195449844882\n",
      "Precision: 0.20833333333333334\n",
      "Recall: 0.42410714285714285\n",
      "(19337, 200)\n"
     ]
    }
   ],
   "source": [
    "# Different techniques for determining feature importance and final set of features to pick # \n",
    "# Conclusion - These feature selection techniques are not working very well as compared to the original techniques\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Lasso Regression for feature selection #\n",
    "# print (training_examples.shape)\n",
    "# lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(training_examples, training_labels)\n",
    "# model = SelectFromModel(lsvc, prefit=True)\n",
    "# selected = model.get_support(indices = False)\n",
    "\n",
    "\n",
    "# lasso_selected_features = []\n",
    "# for i in range(len(training_examples.columns)):\n",
    "#     column1 = training_examples.columns[i] \n",
    "#     column2 = selected[i]\n",
    "#     if column2 == True:\n",
    "#         lasso_selected_features.append(column1)\n",
    "        \n",
    "# print (lasso_selected_features)    \n",
    "\n",
    "# training_examples = training_examples[lasso_selected_features]\n",
    "# print (training_examples.shape) #98 features - reduced precision and recall\n",
    "\n",
    "#VIF \n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from patsy import dmatrices\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# training_examples1 = training_examples \n",
    "# #gather features\n",
    "# features = \"+\".join(training_examples1.columns)\n",
    "# training_examples1['CustomerAdd'] = training_labels\n",
    "# # get y and X dataframes based on this regression:\n",
    "# y, X = dmatrices('CustomerAdd ~' + features, training_examples1, return_type='dataframe')\n",
    "\n",
    "# # For each X, calculate VIF and save in dataframe\n",
    "# vif = pd.DataFrame()\n",
    "# vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "# vif[\"features\"] = X.columns\n",
    "\n",
    "# vif_selected_columns = []\n",
    "# for i in zip(vif[\"features\"],vif[\"VIF Factor\"]):\n",
    "#     if (i[1] <= 5) :\n",
    "#         vif_selected_columns.append(i[0])\n",
    "\n",
    "# vif_selected_columns.remove('Intercept');\n",
    "\n",
    "# training_examples = training_examples[vif_selected_columns]\n",
    "# print (training_examples.shape)\n",
    "\n",
    "# Random forests for feature importance - try seperately(lower recall and AUC) # \n",
    "# then try with Lasso (precision increases by 0.01 but lower recall and AUC) #\n",
    "# Random forest + linear correlation () # \n",
    "\n",
    "# TODO - Need to compare performances of different params for random forests \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_examples, training_labels, random_state=2, test_size= 0.2)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth = 10)\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"Accuracy is:\", accuracy_score(y_test, y_predict))\n",
    "print (\"Precision:\", sklearn.metrics.precision_score(y_test, y_predict))\n",
    "print (\"Recall:\", sklearn.metrics.recall_score(y_test, y_predict))\n",
    "\n",
    "\n",
    "random_forest_feat_imp = clf.feature_importances_\n",
    "random_forest_selected_features = []\n",
    "\n",
    "for i in range(len(training_examples.columns)):\n",
    "    feature = training_examples.columns[i]\n",
    "    value = random_forest_feat_imp[i]\n",
    "    random_forest_selected_features.append((feature,value))\n",
    "     \n",
    "random_forest_selected_features = sorted(random_forest_selected_features,key=lambda x: x[1], reverse = True)\n",
    "random_forest_selected_features = random_forest_selected_features[:200]\n",
    "\n",
    "rf_selected_features = []\n",
    "for i  in random_forest_selected_features:\n",
    "    rf_selected_features.append(i[0])\n",
    "\n",
    "training_examples = training_examples[rf_selected_features]\n",
    "print (training_examples.shape)\n",
    "\n",
    "# Recursive feature elimination to reduce the set further - taking too long #\n",
    "# svc = SVC(kernel=\"linear\", C=1) # TODO - set kernel to rbf to capture non linear interactions?\n",
    "# rfe  =  RFE(estimator=svc, n_features_to_select=50, step=1)\n",
    "# rfe.fit(training_examples, training_labels)\n",
    "# rfe.support_\n",
    "# rfe.ranking_\n",
    "\n",
    "# Could do PCA - but lose interpretability #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WindowsCoreNonM365EnterpriseMean', 'OnPremMean', 'GeoMapping',\n",
      "       'WindowsCoreNonM365E3Mean', 'O365E3add', 'IsFastTrackTenant',\n",
      "       'DesktopSubscription', 'OfficePaidAvailableUnits', 'IntuneEnabledPerc',\n",
      "       'O365E3Trend', 'o365E3Perc', 'HasEMSSku',\n",
      "       'WindowsCoreNonM365E3YoYChange', 'o365E4Perc', 'o365E12Perc',\n",
      "       'IndustryMapping', 'EMSE3', 'HasM365SKUE3', 'AzureMean',\n",
      "       'YammerEnabledPerc', 'EnterpriseMobilityCoreNonM365YoYChange',\n",
      "       'ProplusUsageEnabledPerc', 'EMSPaidAvailableUnits',\n",
      "       'IntuneUsagePaidPerc', 'EMSE3CaoTotalEMS', 'OfficePaidTotalPerc',\n",
      "       'emse3perc', 'O365E4', 'HasWindowsSku', 'O365E4Mean',\n",
      "       'Intune_configured_users', 'AIPUsageEnabledPerc',\n",
      "       'O365E5SecurityAnalyticsMean', 'Intune_active_users', 'M365E3Mean',\n",
      "       'EMSE3FuslTotalEMS', 'HasOfficeSKUE1', 'HasDynamicsSku', 'O365E4Trend',\n",
      "       'O365E5Mean', 'o365e5Perc', 'WindowsPaidTotalPerc', 'AADPP2EnabledPerc',\n",
      "       'IntuneClientMean', 'MobIdentity', 'MCASPaidAvailableUnits', 'Intune',\n",
      "       'EMSE5EnabledUsers', 'MCASEnabledPerc', 'WDATPEnabledUsers',\n",
      "       'CloudAppSecurityMean', 'WDATPPaidAvailableUnits'],\n",
      "      dtype='object')\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# Check if you want to do this - Check correlations with output variable and if p value < 0.05, remove #\n",
    "# Caveat - You are fitting a linear model, there could be non linear correlations you are missing out on? # \n",
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, Y, sl, columns):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "SL = 0.05 # see if this should be 0.05 or 0.06\n",
    "selected_columns = training_examples.columns\n",
    "data_modeled, selected_columns = backwardElimination(training_examples.iloc[:,:].values, training_labels, SL, selected_columns)\n",
    "print (selected_columns)\n",
    "print (len(selected_columns))\n",
    "training_examples = pd.DataFrame(data = data_modeled, columns = selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15469, 53)\n",
      "(3868, 52)\n",
      "(16000, 52)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train - test split and over/under sampling #\n",
    "\n",
    "# TODO - only undersample, check distribtuion of oversampling #\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_examples, training_labels, random_state=2, test_size= 0.2)\n",
    "\n",
    "X_train[['CustomerAdd']] = y_train\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "\n",
    "not_add = X_train[X_train.CustomerAdd == 0]\n",
    "add = X_train[X_train.CustomerAdd == 1]\n",
    "\n",
    "# upsample minority\n",
    "add_upsampled = resample(add,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples= 6000, # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# downsample majority\n",
    "not_add_downsampled = resample(not_add,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = 10000, # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "\n",
    "X_resampled = pd.concat([add_upsampled, not_add_downsampled])\n",
    "\n",
    "y_resampled = X_resampled[['CustomerAdd']] \n",
    "X_resampled = X_resampled.drop('CustomerAdd', axis=1)\n",
    "print (X_resampled.shape)\n",
    "print (y_resampled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No need to run -  Metrics for random and majority classifier #\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils import check_X_y\n",
    "\n",
    "a = y_test[y_test.CustomerAdd == 0]\n",
    "print (a.shape)\n",
    "a = y_test[y_test.CustomerAdd == 1]\n",
    "print (a.shape)\n",
    "\n",
    "\n",
    "# # 50% accuracy, 0.03 precision, recall # \n",
    "\n",
    "# # Majority # \n",
    "# # 97% accuracy, 0 precision, 0 recall #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('gb', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, ...\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=[1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different models # \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import copy\n",
    "\n",
    "# # # Logistic Regression \n",
    "# clf = LogisticRegression(random_state=0, solver='lbfgs',)\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # SVM \n",
    "# clf = SVC(gamma='auto', kernel = 'rbf')\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # GBM\n",
    "# clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 3)  \n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # ADA boost\n",
    "# clf = AdaBoostClassifier(n_estimators = 1000)\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Neural Network - not working out \n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation = 'relu', learning_rate = 'invscaling')\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# GAM - not better than GBM\n",
    "# from pygam import LogisticGAM, GammaGAM\n",
    "# clf = LogisticGAM()\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# XG Boost \n",
    "# clf = XGBClassifier(max_depth=3,learning_rate=0.1, n_estimators=100)\n",
    "# clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Voting classifier - hard voting does not improve performance \n",
    "clf1 = GradientBoostingClassifier(max_depth = 3)\n",
    "#clf2 = AdaBoostClassifier()\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf = VotingClassifier(estimators=[('gb', clf1), ('xgb', clf3)], voting='soft', weights = [1,1])\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# feature_importances = []\n",
    "# for i in zip(clf.feature_importances_, X_resampled.columns):\n",
    "#     feature_importances.append(i)\n",
    "    \n",
    "# feature_importances = sorted(feature_importances,key=lambda x: x[0], reverse = True)\n",
    "\n",
    "\n",
    "# feature_imp_file = open(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\Tries\\\\FeatureImportance.csv\",\"w\", newline = '')\n",
    "# writer = csv.writer(feature_imp_file)\n",
    "# writer.writerow([\"Feature\",\"Importance\"])\n",
    "# for i in feature_importances:\n",
    "#     row = [i[1], i[0]]\n",
    "#     writer.writerow(row)\n",
    "# feature_imp_file.close()    \n",
    "# print (feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8466907962771458\n",
      "Precision: 0.19803600654664485\n",
      "Recall: 0.5401785714285714\n",
      "               Predicted Negative  Predicted Positive\n",
      "True Negative                3154                 490\n",
      "True Positive                 103                 121\n",
      "3154 490 103 121\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-089883f83878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Writing out the probabilities for lift calculation #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtraining_examples_tpid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Predicted\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#Add probability of prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2586\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   2758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2760\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2761\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2762\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m   3119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3121\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of values does not match length of '\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "# Model Performance Metrics # \n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"Accuracy is:\", accuracy_score(y_test, y_predict))\n",
    "print (\"Precision:\", sklearn.metrics.precision_score(y_test, y_predict))\n",
    "print (\"Recall:\", sklearn.metrics.recall_score(y_test, y_predict))\n",
    "\n",
    "#Confusion Matrix\n",
    "print (pandas.DataFrame(\n",
    "    confusion_matrix(y_test, y_predict),\n",
    "    columns=['Predicted Negative', 'Predicted Positive'],\n",
    "    index=['True Negative', 'True Positive']\n",
    "))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "print (tn, fp, fn, tp)\n",
    "\n",
    "#Finding AUC and plotting ROC curve for positive class\n",
    "y_proba_new = []\n",
    "for i in y_proba:\n",
    "    y_proba_new.append(i[1])\n",
    "y_proba_new = np.array(y_proba_new)\n",
    "\n",
    "y_test_new = []\n",
    "\n",
    "y_test_new = y_test[\"CustomerAdd.x\"]\n",
    "y_test_new = np.array(y_test_new)\n",
    "\n",
    "# Writing out the probabilities of the test set for lift calculation #\n",
    "row_index = np.array(y_test.index)\n",
    "test_tpids = []\n",
    "for index in row_index:\n",
    "    test_tpids.append(training_examples_tpid.iloc[index][\"MSSalesID\"])\n",
    "\n",
    "#Add probability of prediction \n",
    "prob0 = []\n",
    "prob1 = []\n",
    "for prob in y_proba:\n",
    "    prob0.append(prob[0])\n",
    "    prob1.append(prob[1])\n",
    "    #max_prob = np.max(prob)\n",
    "    #final_prob.append(max_prob)\n",
    "\n",
    "#print (prob0)\n",
    "#print (prob1)    \n",
    "test_tpids[\"Probability0\"] = prob0\n",
    "test_tpids[\"Probability1\"] = prob1\n",
    "\n",
    "test_tpids.to_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\ModelOutputTraining.csv\", sep=',')\n",
    "\n",
    "\n",
    "# AUC curve \n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_new, y_proba_new)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "#Plotting\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve \n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_new, y_proba_new)\n",
    "#auc = auc(recall, precision)\n",
    "f1 = f1_score(y_test, y_predict)\n",
    "ap = average_precision_score(y_test_new, y_proba_new)\n",
    "auc = auc(recall, precision)\n",
    "print (f1, ap, auc)\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(recall, precision, 'b',\n",
    "label='AUC = %0.2f'% auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,0],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_lift_curve(y_true = y_test_new, y_probas = y_proba)\n",
    "skplt.metrics.plot_cumulative_gain(y_test_new, y_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\Tries\\\\Model_best3.sav'\n",
    "pickle.dump(clf, open(filename, 'wb')) #6000, 10000, voting classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WindowsCoreNonM365EnterpriseMean', 'OnPremMean', 'GeoMapping',\n",
      "       'WindowsCoreNonM365E3Mean', 'O365E3add', 'IsFastTrackTenant',\n",
      "       'DesktopSubscription', 'OfficePaidAvailableUnits', 'IntuneEnabledPerc',\n",
      "       'O365E3Trend', 'o365E3Perc', 'HasEMSSku',\n",
      "       'WindowsCoreNonM365E3YoYChange', 'o365E4Perc', 'o365E12Perc',\n",
      "       'IndustryMapping', 'EMSE3', 'HasM365SKUE3', 'AzureMean',\n",
      "       'YammerEnabledPerc', 'EnterpriseMobilityCoreNonM365YoYChange',\n",
      "       'ProplusUsageEnabledPerc', 'EMSPaidAvailableUnits',\n",
      "       'IntuneUsagePaidPerc', 'EMSE3CaoTotalEMS', 'OfficePaidTotalPerc',\n",
      "       'emse3perc', 'O365E4', 'HasWindowsSku', 'O365E4Mean',\n",
      "       'Intune_configured_users', 'AIPUsageEnabledPerc',\n",
      "       'O365E5SecurityAnalyticsMean', 'Intune_active_users', 'M365E3Mean',\n",
      "       'EMSE3FuslTotalEMS', 'HasOfficeSKUE1', 'HasDynamicsSku', 'O365E4Trend',\n",
      "       'O365E5Mean', 'o365e5Perc', 'WindowsPaidTotalPerc', 'AADPP2EnabledPerc',\n",
      "       'IntuneClientMean', 'MobIdentity', 'MCASPaidAvailableUnits', 'Intune',\n",
      "       'EMSE5EnabledUsers', 'MCASEnabledPerc', 'WDATPEnabledUsers',\n",
      "       'CloudAppSecurityMean', 'WDATPPaidAvailableUnits'],\n",
      "      dtype='object')\n",
      "52\n",
      "[[0.56837079 0.43162921]\n",
      " [0.91587868 0.08412132]\n",
      " [0.61619036 0.38380964]\n",
      " ...\n",
      " [0.73652029 0.26347972]\n",
      " [0.59517289 0.40482711]\n",
      " [0.93943553 0.06056446]]\n"
     ]
    }
   ],
   "source": [
    "#Scoring with final model\n",
    "\n",
    "testing_examples = pandas.read_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Data\\\\Features\\\\Scoring\\\\AllFeaturesv4.csv\")\n",
    "testing_examples = testing_examples.drop('ym', axis=1)\n",
    "testing_examples_tpid = testing_examples[[\"FinalTPID\"]]\n",
    "testing_examples = testing_examples.drop('FinalTPID', axis=1)\n",
    "\n",
    "print (selected_columns)\n",
    "print (len(selected_columns))\n",
    "\n",
    "testing_examples_new = testing_examples[selected_columns]\n",
    "\n",
    "\n",
    "test_predict = clf.predict(testing_examples_new)\n",
    "test_proba = clf.predict_proba(testing_examples_new)\n",
    "\n",
    "\n",
    "# print (test_predict)\n",
    "# print (test_proba)\n",
    "\n",
    "#Add predicted label \n",
    "testing_examples_tpid[\"Predicted\"] = test_predict\n",
    "\n",
    "print (test_proba)\n",
    "#Add probability of prediction \n",
    "prob0 = []\n",
    "prob1 = []\n",
    "for prob in test_proba:\n",
    "    prob0.append(prob[0])\n",
    "    prob1.append(prob[1])\n",
    "    #max_prob = np.max(prob)\n",
    "    #final_prob.append(max_prob)\n",
    "\n",
    "#print (prob0)\n",
    "#print (prob1)    \n",
    "testing_examples_tpid[\"Probability0\"] = prob0\n",
    "testing_examples_tpid[\"Probability1\"] = prob1\n",
    "\n",
    "testing_examples_tpid.to_csv(\"C:\\\\Users\\\\varamase\\\\Documents\\\\DataStreams\\\\M365NCA\\\\Modeling\\\\ModelOutput.csv\", sep=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
