{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minmaxnorm(data):\n",
    "    X = np.array(data)\n",
    "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n",
      "8075789\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "bm25_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\bm25_features.p\", \"rb\" ))\n",
    "qid_docid_rel_bm25 = []\n",
    "scores = []\n",
    "for key,value in bm25_features.items():\n",
    "    qid_docid_rel_bm25.append(key)\n",
    "    scores.append(value)\n",
    "\n",
    "bm25_scaled = minmaxnorm(scores)\n",
    "bm25_scaled_dict = defaultdict(list)\n",
    "for key,value in zip(qid_docid_rel_bm25, bm25_scaled):\n",
    "    bm25_scaled_dict[key].append(value)\n",
    "\n",
    "print (len(scores))\n",
    "print (len(bm25_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n"
     ]
    }
   ],
   "source": [
    "coverage_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\coverage_features.p\", \"rb\" ))\n",
    "qid_docid_rel_cov = []\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "for key,value in coverage_features.items():\n",
    "    qid_docid_rel_cov.append(key)\n",
    "    scores_1.append(value[0])\n",
    "    scores_2.append(value[1])\n",
    "    \n",
    "coverage_scaled_1 = minmaxnorm(scores_1)\n",
    "coverage_scaled_2 = minmaxnorm(scores_2)\n",
    "\n",
    "coverage_scaled_dict = defaultdict(list)\n",
    "for key,value1,value2 in zip(qid_docid_rel_cov, coverage_scaled_1, coverage_scaled_2):\n",
    "    coverage_scaled_dict[key].append(value1)\n",
    "    coverage_scaled_dict[key].append(value2)\n",
    "\n",
    "print (len(coverage_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n"
     ]
    }
   ],
   "source": [
    "doclen_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\doclen_features.p\", \"rb\" ))\n",
    "qid_docid_rel_doclen = []\n",
    "scores = []\n",
    "for key,value in doclen_features.items():\n",
    "    qid_docid_rel_doclen.append(key)\n",
    "    scores.append(value)\n",
    "\n",
    "doclen_scaled = minmaxnorm(scores)\n",
    "\n",
    "doclen_scaled_dict = defaultdict(list)\n",
    "for key,value in zip(qid_docid_rel_doclen, doclen_scaled):\n",
    "    doclen_scaled_dict[key].append(value)\n",
    "\n",
    "print (len(doclen_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n"
     ]
    }
   ],
   "source": [
    "idf_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\idf_features.p\", \"rb\" ))\n",
    "qid_docid_rel_idf = []\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "scores_3 = []\n",
    "for key,value in idf_features.items():\n",
    "    qid_docid_rel_idf.append(key)\n",
    "    scores_1.append(value[0])\n",
    "    scores_2.append(value[1])\n",
    "    scores_3.append(value[2])\n",
    "    \n",
    "idf_scaled_1 = minmaxnorm(scores_1)\n",
    "idf_scaled_2 = minmaxnorm(scores_2)\n",
    "idf_scaled_3 = minmaxnorm(scores_3)\n",
    "\n",
    "idf_scaled_dict = defaultdict(list)\n",
    "for key,value1,value2,value3 in zip(qid_docid_rel_idf, idf_scaled_1, idf_scaled_2,idf_scaled_3):\n",
    "    idf_scaled_dict[key].append(value1)\n",
    "    idf_scaled_dict[key].append(value2)\n",
    "    idf_scaled_dict[key].append(value3)\n",
    "\n",
    "print (len(idf_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n"
     ]
    }
   ],
   "source": [
    "tf_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\tf_features.p\", \"rb\" ))\n",
    "qid_docid_rel_tf = []\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "scores_3 = []\n",
    "for key,value in tf_features.items():\n",
    "    qid_docid_rel_tf.append(key)\n",
    "    scores_1.append(value[0])\n",
    "    scores_2.append(value[1])\n",
    "    scores_3.append(value[2])\n",
    "    \n",
    "tf_scaled_1 = minmaxnorm(scores_1)\n",
    "tf_scaled_2 = minmaxnorm(scores_2)\n",
    "tf_scaled_3 = minmaxnorm(scores_3)\n",
    "\n",
    "tf_scaled_dict = defaultdict(list)\n",
    "for key,value1,value2,value3 in zip(qid_docid_rel_tf, tf_scaled_1, tf_scaled_2,tf_scaled_3):\n",
    "    tf_scaled_dict[key].append(value1)\n",
    "    tf_scaled_dict[key].append(value2)\n",
    "    tf_scaled_dict[key].append(value3)\n",
    "print (len(tf_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8075789\n"
     ]
    }
   ],
   "source": [
    "tfidf_features = pickle.load(open( \"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\tfidf_features.p\", \"rb\" ))\n",
    "qid_docid_rel_tfidf = []\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "scores_3 = []\n",
    "scores_4 = []\n",
    "for key,value in tfidf_features.items():\n",
    "    qid_docid_rel_tfidf.append(key)\n",
    "    scores_1.append(value[0])\n",
    "    scores_2.append(value[1])\n",
    "    scores_3.append(value[2])\n",
    "    scores_4.append(value[3])\n",
    "    \n",
    "tfidf_scaled_1 = minmaxnorm(scores_1)\n",
    "tfidf_scaled_2 = minmaxnorm(scores_2)\n",
    "tfidf_scaled_3 = minmaxnorm(scores_3)\n",
    "tfidf_scaled_4 = minmaxnorm(scores_4)\n",
    "\n",
    "tfidf_scaled_dict = defaultdict(list)\n",
    "for key,value1,value2,value3,value4 in zip(qid_docid_rel_tfidf, tfidf_scaled_1, tfidf_scaled_2,tfidf_scaled_3,tfidf_scaled_4):\n",
    "    tfidf_scaled_dict[key].append(value1)\n",
    "    tfidf_scaled_dict[key].append(value2)\n",
    "    tfidf_scaled_dict[key].append(value3)\n",
    "    tfidf_scaled_dict[key].append(value4)\n",
    "print (len(tfidf_scaled_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e1bfa9887d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" #\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#combine all features into feature file\n",
    "# format -> 3 qid:1 1:1 2:1 3:0 4:0.2 5:0 #1A\n",
    "f = open(\"C:\\\\Users\\\\mapyredd\\\\Documents\\\\marco\\\\data\\\\LTR\\\\train.features\", \"w\")\n",
    "count = 0\n",
    "qid_docid_rel_sorted = sorted(qid_docid_rel_bm25, key=lambda x:x[0])\n",
    "for key in qid_docid_rel_sorted:\n",
    "    all_features = bm25_scaled_dict[key] + doclen_scaled_dict[key]  + coverage_scaled_dict[key] + tf_scaled_dict[key] + idf_scaled_dict[key] + tfidf_scaled_dict[key]\n",
    "    f.write(str(key[2]) + \" qid:\" + str(key[0]))\n",
    "    if len(all_features) != 14:\n",
    "        import pdb;pdb.set_trace()\n",
    "    for i in range(len(all_features)):\n",
    "        f.write(\" \" + str(i+1) + \":\" + str(all_features[i]))\n",
    "    f.write(\" #\" + str(key[1]) + \"\\n\")\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tqid:1136966\t1:0.11600578550246791\t2:0.13966480446927373\t3:0.16666666666666666\t4:0.5454545454545455\t5:0.08333333333333333\t6:0.07142857142857142\t7:0.0\t8:0.10991390868009636\t9:0.26964156695339175\t10:0.0\t11:0.10325890093003738\t12:0.4136800979665728\t13:0.0\t14:0.019896097796729385\t#989211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#key = (1136966, 88416, 1)\n",
    "#key = (1136966, 968411, 1)\n",
    "key = (1136966, 989211, 1)\n",
    "all_features = bm25_scaled_dict[key] + doclen_scaled_dict[key]  + coverage_scaled_dict[key] + tf_scaled_dict[key] + idf_scaled_dict[key] + tfidf_scaled_dict[key]\n",
    "all_features = [str(f) for f in all_features]\n",
    "a = str(key[2]) + \"\\tqid:\" + str(key[0])\n",
    "for i in range(len(all_features)):\n",
    "    a += \"\\t\" + str(i+1) + \":\" + str(all_features[i])\n",
    "a += \"\\t#\" + str(key[1]) + \"\\n\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
